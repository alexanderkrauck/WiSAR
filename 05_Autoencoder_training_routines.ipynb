{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import utils.architectures as architectures\n",
    "from utils.basic_function import draw_labels\n",
    "import utils.data as data\n",
    "import utils.sub_architectures as sub_architectures\n",
    "\n",
    "#__import__('3_project.evaluate')\n",
    "#os.chdir('3_project')\n",
    "\n",
    "import utils.evaluation_metrics as evaluation_metrics\n",
    "\n",
    "#evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TRAINING UTILS / PLOTTING FUNCTIONS\n",
    "\n",
    "### Trains a (sub-architecture) autoencoder given Model type\n",
    "def train_subarchitecture(\n",
    "    model, \n",
    "    optim, \n",
    "    lr, \n",
    "    num_epochs, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    val_data,\n",
    "    device,\n",
    "    saved_model_filename,\n",
    "    plot_training=False, \n",
    "    save_model=True,\n",
    "):\n",
    "\n",
    "    device = get_device()\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    criterion = nn.MSELoss() #Loss function\n",
    "    optimizer = optim(model.parameters(), lr=lr) #Optimizer\n",
    "\n",
    "    lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"min\", \n",
    "        factor=0.1, \n",
    "        patience=2, \n",
    "        cooldown=1, \n",
    "        verbose=True)\n",
    "\n",
    "    # Training\n",
    "    train_loss = []\n",
    "    model.train()\n",
    "    \n",
    "    print(\"Training Progress: \")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        for x in tqdm(train_loader):\n",
    "\n",
    "            x = x.to(device)\n",
    "            outputs = model(x)\n",
    "\n",
    "            loss = criterion(outputs, x)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        train_loss.extend(epoch_losses)\n",
    "        lr_schedule.step(np.mean(epoch_losses))\n",
    "        \n",
    "        if(plot_training):\n",
    "            plt.plot(range(len(train_loss)), train_loss)\n",
    "            plt.show()\n",
    "            print(f\"\\rLoss After {epoch+1} Epochs: \"+str(np.mean(epoch_losses)))\n",
    "            if np.mean(epoch_losses) < 0.01:\n",
    "                break\n",
    "\n",
    "        if(save_model):\n",
    "            model.save(filename=saved_model_filename)\n",
    "\n",
    "    return model\n",
    "\n",
    "### Plots \"n\" reconstructed images side-by-side with the original images\n",
    "def show_n_reconstructed_images(model, output_images, n:int):\n",
    "\n",
    "    numpy_images, numpy_output = output_images\n",
    "\n",
    "    print(\"\\nRendering Results...\")\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=n, ncols=3, figsize=(10,5*n))\n",
    "    for idx in range(n):\n",
    "        axes[idx, 0].imshow(numpy_images[idx])\n",
    "        axes[idx, 0].set_title(\"Original Image\")\n",
    "        axes[idx, 1].set_title(\"Reconstructed Image\")\n",
    "        axes[idx, 1].imshow(numpy_output[idx])\n",
    "        axes[idx, 1].set_title(\"Reconstructed Image\")\n",
    "        axes[idx, 2].imshow(abs(numpy_images[idx]-numpy_output[idx]))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "### Plots \"n\" Bounding Box predictions overlaying the target image\n",
    "def show_n_bounding_box_results(approach, dataset, n:int):\n",
    "\n",
    "    for i in range(n):\n",
    "        print(f\"Sample Nr. {i}:\")\n",
    "        sample = dataset[i]\n",
    "        samples = [sample]\n",
    "\n",
    "        res = approach.infer(samples, verbose=0, threshold=0.1)\n",
    "        print(res)\n",
    "\n",
    "        mid_img = sample.get_warped_photo(3,8)\n",
    "        draw_labels(mid_img, [res[0], sample.labels])\n",
    "\n",
    "### Returns a dataset depending on Architecture / Sub-Architecture training\n",
    "def get_dataset(sub):\n",
    "    if(sub):\n",
    "        return data.RandomSamplingGridCutoutDataset(preprocess_image_options = {\n",
    "            \"use_mask\": True,\n",
    "            \"equalize_hist\": False,\n",
    "            \"crop_black\": True,\n",
    "            \"match_histogram\": False\n",
    "        }, crop_shape=(128,128), resample_image_every_n_draws=50)\n",
    "    else:\n",
    "        return data.MultiViewTemporalDataset(mode = \"validation\", preprocess_image_options = {\n",
    "            \"use_mask\": True,\n",
    "            \"equalize_hist\": False,\n",
    "            \"crop_black\": False,\n",
    "            \"match_histogram\": False\n",
    "        }, data_path=\"data\")\n",
    "\n",
    "### Evaluates the model for the Sub-Architecture case exclusively\n",
    "def evaluate_model(model, train_loader, val_loader, device):\n",
    "    images = next(iter(val_loader))\n",
    "    numpy_images = images.numpy()\n",
    "\n",
    "    #Sample outputs\n",
    "    model.eval()\n",
    "    numpy_output = model(images.to(device)).detach().cpu().numpy()\n",
    "\n",
    "    numpy_images = np.transpose(numpy_images, (0, 2, 3, 1))\n",
    "    numpy_output = np.transpose(numpy_output, (0, 2, 3, 1))\n",
    "\n",
    "    return numpy_images, numpy_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss After 1 Epochs: 0.008027204\n",
      "\n",
      "Model Type:  <class 'utils.sub_architectures.ConvolutionalAutoencoderV2'>  Optimizer:  <class 'torch.optim.adam.Adam'>  Learning rate:  0.01  Epochs:  10\n",
      "Device: NVIDIA GeForce GTX 1060 6GB\n",
      "Inferring on validation dataset, this may take some time...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26604/2248553642.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    145\u001b[0m                     \u001b[1;31m#scores.append([model_type, optim, lr, epochs, score])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m \u001b[0mHyperparameterSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;31m#trained_model_sub = train(pretrained_sub_arch=None, show_results=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26604/2248553642.py\u001b[0m in \u001b[0;36mHyperparameterSearch\u001b[1;34m()\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nModel Type: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_types'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" Optimizer: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'optimizers'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" Learning rate: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'learning_rates'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" Epochs: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epoch_values'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_sub_arch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrained_model_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_results\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_inference_to_json\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_from_saved\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_types'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'optimizers'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'learning_rates'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epoch_values'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26604/2248553642.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(pretrained_sub_arch, show_results, load_from_saved, save_inference_to_json, scoring, model, optim, lr, num_epochs)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_inference_to_json\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Inferring on validation dataset, this may take some time...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapproach_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[0mbox_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mbox_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\JKU\\ComputerVision\\Project\\WiSAR\\utils\\architectures.py\u001b[0m in \u001b[0;36minfer\u001b[1;34m(self, samples, threshold, verbose, write_boxes_to_file)\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0marchitecture\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_architectures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m                 \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marchitecture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m                     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\JKU\\ComputerVision\\Project\\WiSAR\\utils\\architectures.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, sample, verbose)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;31m#timesteps x perspectives x height x width x color -> timesteps * perspectives x height x width x color\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         \u001b[0mflat_differences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdifferences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mdifferences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[0mflat_differences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mflat_differences\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mflat_homographies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhomographies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhomographies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### RUN TRAINING ROUTINES #####\n",
    "\n",
    "def train(pretrained_sub_arch: sub_architectures.AbstractTorchArchitecture, show_results=True, load_from_saved=False, save_inference_to_json=False, scoring=False,\n",
    "model=sub_architectures.ConvolutionalAutoencoderV2, optim=torch.optim.Adam, lr=1e-3, num_epochs=25):\n",
    "    \"\"\"\n",
    "    :param AbstractTorchArchitecture pretrained_sub_arch: Pretrained sub-architecture, None if training a sub-architecture\n",
    "    :param show_results bool: If True results will be displayed\n",
    "    :param load_from_saved bool: If True loads pre-trained autoencoder from the \"state_dict\" specified below, otherwise uses uses the pretrained sub-architecture\n",
    "    :param save_inference_to_json bool: If True performs inference on whole dataset and saves bounding box predictions to JSON file\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get PyTorch \"device\"\n",
    "    device=get_device()\n",
    "    if(torch.cuda.is_available()):\n",
    "        print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        print(\"Using CPU: consider using GPU for increased performance\")\n",
    "\n",
    "    # Prepare dataset\n",
    "    #sub = True #Set \"sub\" to True to get val_data for Sub-Architectures, False for Architectures\n",
    "    if(pretrained_sub_arch is None):\n",
    "        dataset = get_dataset(sub=True)\n",
    "    else:\n",
    "        dataset = get_dataset(sub=False)\n",
    "\n",
    "    # Dataloader(s)\n",
    "    train_loader = val_loader = torch.utils.data.DataLoader(dataset, batch_size=32, num_workers=0, shuffle=True)\n",
    "\n",
    "    if(pretrained_sub_arch is None):\n",
    "\n",
    "        ### SUB-ARCHITECTURE TRAINING ROUTINE / MODEL ANALYTICS ###\n",
    "\n",
    "        trained_model = train_subarchitecture(                    # HYPERPARAMETERS: \n",
    "            model=model,                                        # Untrained model\n",
    "            optim=optim,                                          # Optimizer\n",
    "            lr=lr,                                                # Learning rate\n",
    "            num_epochs=num_epochs,                                # Epochs\n",
    "                                                                \n",
    "                                                                  # DATALOADERS / OPTIONS:\n",
    "            train_loader=train_loader,                            # Training Dataloader\n",
    "            val_loader=val_loader,                                # Validation Dataloader\n",
    "            val_data=dataset,                                     # Dataset(?)\n",
    "            device=device,                                        # PyTorch \"device\"\n",
    "            saved_model_filename=\"no_histogram_128_trained\",      # Model Filename\n",
    "            plot_training=True,                                   # Plot loss during training\n",
    "            save_model=True                                       # Save model after training\n",
    "        )    \n",
    "\n",
    "        if(show_results):\n",
    "            output_images = evaluate_model(trained_model, trained_model, val_loader, device) #Evaluate model (Get image results)\n",
    "            show_n_reconstructed_images(trained_model, output_images, n=3)\n",
    "\n",
    "        return trained_model\n",
    "\n",
    "    else:\n",
    "        ### MAIN ARCHITECTURE TRAINING ROUTINE ###\n",
    "\n",
    "        autoencoder = sub_architectures.ConvolutionalAutoencoderV2()\n",
    "        #load_from_saved = False #True: Imports model from below filename, False: Uses \"pretrained_sub_arch\" argument as model\n",
    "\n",
    "        if(load_from_saved):\n",
    "            state_dict = torch.load(\"saved_models/ConvolutionalAutoencoderV2/no_histogram_128_trained\", map_location=device)\n",
    "            autoencoder.load_state_dict(state_dict)\n",
    "        else:\n",
    "            autoencoder = pretrained_sub_arch\n",
    "\n",
    "        #TODO: This might need some formatting..\n",
    "\n",
    "        importlib.reload(architectures)\n",
    "        approach = architectures.BasicAutoencoderAnomalyDetectionV1(autoencoder, device=device, image_sizes=(1024,1024)) #Load in autoencoder\n",
    "        approach_score = architectures.ScoreEnsembleAnomalyDetection([approach], [1])\n",
    "\n",
    "        if(show_results):\n",
    "            print(show_results)\n",
    "            print(\"Showing results\")\n",
    "            show_n_bounding_box_results(approach_score, dataset, 5)\n",
    "        \n",
    "        if(save_inference_to_json):\n",
    "            print(\"Inferring on validation dataset, this may take some time...\")                                                       \n",
    "            res = approach_score.infer(dataset, threshold=0.1)\n",
    "            box_list = [el.tolist() for el in res]\n",
    "            box_dict = {}\n",
    "            \n",
    "            for sample, boxes in zip(dataset, box_list):\n",
    "                sample_name = os.path.split(sample.sample_path)[-1]\n",
    "                box_dict[sample_name] = boxes\n",
    "            \n",
    "            with open(\"validation.json\", \"w\") as fi:\n",
    "                json.dump(box_dict, fi)\n",
    "\n",
    "        if(scoring):\n",
    "            with open(\"validation.json\") as fi:\n",
    "                preds = json.load(fi)\n",
    "            with open(\"data/validation/labels.json\") as fi:\n",
    "                targets = json.load(fi)\n",
    "\n",
    "            ap = evaluation_metrics.evaluate(preds, targets)\n",
    "            return ap\n",
    "\n",
    "            print(\"Average Precision: \", ap)       \n",
    "\n",
    "def HyperparameterSearch():\n",
    "    \n",
    "    param_grid = {'model_types': [sub_architectures.ConvolutionalAutoencoderV2],\n",
    "                  'optimizers': [torch.optim.Adam, torch.optim.RAdam, torch.optim.Adagrad],\n",
    "                  'learning_rates': [1e-2, 5e-3, 1e-3],\n",
    "                  'epoch_values': [10, 20, 30, 40, 50]\n",
    "                  }\n",
    "    scores = []\n",
    "\n",
    "    for item in list(ParameterGrid(param_grid)):\n",
    "        trained_model_sub = train(pretrained_sub_arch=None,\n",
    "                                  show_results=False,\n",
    "                                  model=item['model_types'](),\n",
    "                                  optim=item['optimizers'],\n",
    "                                  lr=item['learning_rates'],\n",
    "                                  num_epochs=item['epoch_values']\n",
    "                                  )\n",
    "                                  \n",
    "        print(\"\\nModel Type: \", item['model_types'], \" Optimizer: \", item['optimizers'], \" Learning rate: \", item['learning_rates'], \" Epochs: \", item['epoch_values'])\n",
    "        score = train(pretrained_sub_arch=trained_model_sub, show_results=False, save_inference_to_json=True, load_from_saved=False, scoring=True)\n",
    "        scores.append([item['model_types'], item['optimizers'], item['learning_rates'], item['epoch_values'], score])\n",
    "\n",
    "    for score in scores:\n",
    "        print(score)\n",
    "\n",
    "        \n",
    "\n",
    "    #####DEPRECATED CODE#####\n",
    "\n",
    "    #model_types = [sub_architectures.ConvolutionalAutoencoderV2] #NOTE: Only one Model class seems to have the correct \"output size\" currently\n",
    "    #optimizers = [torch.optim.Adagrad, torch.optim.Adam, torch.optim.RAdam]\n",
    "    #learning_rates = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    #epoch_values = [10, 20, 30, 40, 50] #NOTE: All optimizers seem to achieve <0.01 loss quickly so don't go past 1 epoch often, breaking training early...\n",
    "    #scores = []\n",
    "\n",
    "    #for model_type in model_types:\n",
    "        #for optim in optimizers:\n",
    "            #for lr in learning_rates:\n",
    "                #for epochs in epoch_values:\n",
    "                    #trained_model_sub = train(pretrained_sub_arch=None, show_results=False, model=model_type(), optim=optim, lr=lr, num_epochs=epochs)\n",
    "                    #print(\"\\nModel Type: \", model_type, \" Optimizer: \", optim, \" Learning rate: \", lr, \" Epochs: \", epochs)\n",
    "                    #score = train(pretrained_sub_arch=trained_model_sub, show_results=False, save_inference_to_json=True, load_from_saved=False, scoring=True)\n",
    "                    #scores.append([model_type, optim, lr, epochs, score])\n",
    "\n",
    "HyperparameterSearch()\n",
    "\n",
    "#trained_model_sub = train(pretrained_sub_arch=None, show_results=True) \n",
    "#pretrained_sub_arch=None if training a sub-architecture, otherwise exchange \"None\" placeholder with a pre-trained model\n",
    "\n",
    "#trained_model = train(pretrained_sub_arch=trained_model_sub, show_results=False, save_inference_to_json=False, load_from_saved=True, scoring=True)  \n",
    "#NOTE: When training an Architecture for results, set save_inference_to_json=True to infer the validation dataset (costly)\n",
    "#As an alternative, show_results=True to see results from a subset of the dataset\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8be9447539d9575c41e4c0fdb9e82ec5973f32843cf2f527d93a54d8d36b47fb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
